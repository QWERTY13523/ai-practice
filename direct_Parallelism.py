import os
import json
import uuid
import requests
import re
import shutil
import time
import glob
import random
import traceback
import asyncio
import httpx
from fastapi import FastAPI, UploadFile, File, BackgroundTasks, Request, HTTPException
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydub import AudioSegment

from openai import OpenAI

app = FastAPI()
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

# ================= 1. é…ç½®è·¯å¾„ =================
UPLOAD_DIR = "uploads"
OUTPUT_DIR = "outputs"
TEMP_VOICE_DIR = "uploads/custom_voices"
VOICE_POOL_DIR = "/home/nyw/AI-practice/resource/input_audio"
BGM_DIR = "/home/nyw/AI-practice/resource/pre_train_wav/background" 

for d in [UPLOAD_DIR, OUTPUT_DIR, TEMP_VOICE_DIR, VOICE_POOL_DIR, BGM_DIR]:
    os.makedirs(d, exist_ok=True)

# GPU æœåŠ¡åœ°å€
URL_COSY = "http://localhost:8005/generate" 
URL_INDEX = "http://localhost:8002/generate"

TASKS = {}

# ================= 2. åˆå§‹åŒ–å¤§æ¨¡åž‹ =================
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY") or "sk-cfc644272f8b4be2aa58f9b240636083"
client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# ================= 3. å·¥å…·å‡½æ•° =================

def match_target_amplitude(sound, target_dBFS=-20.0):
    change_in_dBFS = target_dBFS - sound.dBFS
    return sound.apply_gain(change_in_dBFS)

def mix_speech_with_bgm(speech_seg, bgm_path):
    """
    å•å¥æ··åˆé€»è¾‘ (ä¿®æ”¹ç‰ˆ)ï¼š
    1. ä¸å¾ªçŽ¯ï¼šBGM åªæ’­æ”¾ä¸€éã€‚
    2. å¦‚æžœ BGM é•¿äºŽäººå£°ï¼šè£å‰ªå¹¶æ·¡å‡ºã€‚
    3. å¦‚æžœ BGM çŸ­äºŽäººå£°ï¼šè‡ªç„¶æ’­æ”¾ç»“æŸã€‚
    """
    if not bgm_path or not os.path.exists(bgm_path):
        return speech_seg 
    
    try:
        bgm = AudioSegment.from_file(bgm_path)
        
        # 1. ç»Ÿä¸€åŸºå‡†éŸ³é‡ & åŽ‹ä½ŽèƒŒæ™¯éŸ³
        bgm = match_target_amplitude(bgm, -20.0)
        bgm = bgm - 12 
        
        # 2. è®¡ç®—ç›®æ ‡é•¿åº¦ (äººå£° + 500ms å°¾éŸµ)
        target_len = len(speech_seg) + 500
        
        # 3. ã€æ ¸å¿ƒä¿®æ”¹ã€‘åªæ’­ä¸€éé€»è¾‘
        if len(bgm) > target_len:
            # Case A: BGM æ¯”äººå£°é•¿ -> è£å‰ªåˆ°äººå£°é•¿åº¦ï¼Œå¹¶åšæ·¡å‡º
            bgm = bgm[:target_len]
            bgm = bgm.fade_out(500)
        else:
            # Case B: BGM æ¯”äººå£°çŸ­ -> ä¸å¾ªçŽ¯ï¼Œä¸å¼ºè¡Œæ·¡å‡º(ä¿ç•™è‡ªç„¶å°¾éŸ³)ï¼Œç›´æŽ¥ç”¨
            pass
            
        # ç»Ÿä¸€åŠ å¼€å¤´æ·¡å…¥ï¼Œé˜²æ­¢çªå…€
        bgm = bgm.fade_in(500)
        
        # 4. å åŠ  (å¦‚æžœ BGM çŸ­ï¼Œoverlay ä¼šè‡ªåŠ¨å¤„ç†ï¼Œä¸ä¼šæŠ¥é”™)
        mixed = speech_seg.overlay(bgm, position=0)
        return mixed

    except Exception as e:
        print(f"âš ï¸ BGMèžåˆå¤±è´¥ [{os.path.basename(bgm_path)}]: {e}")
        return speech_seg

# ================= 4. LLM åˆ†æžé€»è¾‘ =================

def get_all_bgm_filenames():
    files = []
    if os.path.exists(BGM_DIR):
        for f in os.listdir(BGM_DIR):
            if f.lower().endswith(('.mp3', '.wav', '.flac')):
                files.append(f)
    return files

def parse_json_output(text_output):
    print(f"----- LLM åŽŸå§‹è¿”å›ž (å‰100å­—) -----\n{text_output[:100]}...\n-------------------------------")
    clean_text = re.sub(r'```json\s*', '', text_output)
    clean_text = re.sub(r'```', '', clean_text).strip()
    try:
        data = json.loads(clean_text)
        results = []
        for item in data:
            role = item.get("role", item.get("è§’è‰²", "æ—ç™½")).strip()
            emotion = item.get("emotion", item.get("æƒ…ç»ª", "å¹³æ·¡"))
            text = item.get("text", item.get("å°è¯", ""))
            bgm = item.get("bgm", "") 
            if "æ—" in role and "ç™½" in role: role = "æ—ç™½"
            if role.lower() == "narrator": role = "æ—ç™½"
            results.append({"è§’è‰²": role, "æƒ…ç»ª": emotion, "å°è¯": text, "bgm": bgm})
        return results
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æžå¤±è´¥: {e}")
        return []

def analyze_novel_roles_llm(text_content):
    bgm_files = get_all_bgm_filenames()
    bgm_list_str = json.dumps(bgm_files, ensure_ascii=False)
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæœ‰å£°ä¹¦è„šæœ¬åˆ¶ä½œä¸“å®¶ã€‚è¯·å°†è¾“å…¥çš„å°è¯´æ–‡æœ¬æ‹†è§£ä¸º JSON æ•°ç»„ã€‚\n"
        f"å¯ç”¨çš„èƒŒæ™¯éŸ³ä¹/éŸ³æ•ˆåº“å¦‚ä¸‹ï¼š{bgm_list_str}\n\n"
        "ã€æ ¸å¿ƒä»»åŠ¡ã€‘ï¼š\n"
        "å°†å°è¯´åŽŸæ–‡æ‹†è§£ä¸ºé€‚åˆå¤šäººæœ‰å£°å‰§æœ—è¯»çš„è„šæœ¬ã€‚**åŽŸæ–‡çš„æ¯ä¸€ä¸ªå­—ã€æ ‡ç‚¹éƒ½å¿…é¡»ä¿ç•™ï¼Œä¸èƒ½æœ‰ä»»ä½•é—æ¼ï¼**\n\n"
        "ã€æ‹†è§£è§„åˆ™ã€‘ï¼š\n"
        "1. **å¯¹è¯å†…å®¹**ï¼ˆå¼•å·å†…ï¼‰ï¼šåˆ†é…ç»™å¯¹åº”çš„è§’è‰²ã€‚\n"
        "2. **éžå¯¹è¯å†…å®¹**ï¼ˆå¼•å·å¤–ï¼‰ï¼š**å…¨éƒ¨**åˆ†é…ç»™è§’è‰²â€œæ—ç™½â€ã€‚åŒ…æ‹¬åŠ¨ä½œã€ç¥žæ€ã€ä»¥åŠâ€œä»–è¯´â€ã€â€œé“â€ç­‰å¼•å¯¼è¯­ã€‚\n"
        "3. **å¿…é¡»æ‹†åˆ†**ï¼šå½“ä¸€è¡Œæ–‡å­—æ˜¯ [æå†™ + å¯¹è¯] æ—¶ï¼Œå¿…é¡»æ‹†åˆ†ä¸º [æ—ç™½] + [è§’è‰²] ä¸¤æ¡ï¼Œä¸èƒ½åˆå¹¶ï¼\n"
        "4. **æƒ…ç»ªæŽ§åˆ¶**ï¼šæƒ…ç»ª emotion å¿…é¡»å…‹åˆ¶ã€‚å°½é‡ä¸è¦æœ‰æ„¤æ€’ä¹‹ç±»æ¯”è¾ƒæ¿€åŠ¨çš„æƒ…ç»ª\n\n"
        "5. ã€æ—ç™½ç‰¹æ®Šè§„åˆ™ã€‘ï¼šæ—ç™½æ˜¯â€˜è¯´ä¹¦äººâ€™ï¼Œå¿…é¡»æŠ½ç¦»äºŽå‰§æƒ…ä¹‹å¤–ã€‚æ— è®ºå‰§æƒ…å¤šä¹ˆæ¿€çƒˆï¼Œæ—ç™½çš„æƒ…ç»ªåªèƒ½æ˜¯ 'æ²‰ç¨³'ã€'è®²è¿°æ„Ÿ'ã€'èˆ’ç¼“' æˆ– 'å¸¦æœ‰æ‚¬å¿µ'ã€‚ä¸¥ç¦ç»™æ—ç™½åˆ†é… 'æ„¤æ€’'ã€'å“­æ³£'ã€'å¤§ç¬‘' ç­‰å…·ä½“çš„äººç‰©æƒ…ç»ªï¼\n\n"
        "ã€æ‹†åˆ†ç¤ºä¾‹ï¼ˆä¸¥æ ¼æ¨¡ä»¿æ­¤é€»è¾‘ï¼‰ã€‘ï¼š\n"
        "è¾“å…¥åŽŸæ–‡ï¼š\n"
        "çŒªå…«æˆ’ä¸€è§ï¼ŒæŠŠå˜´ä¸€å™˜ï¼Œå˜Ÿå›”é“ï¼šâ€œå¸ˆçˆ¶ï¼Œç³Ÿç³•äº†ï¼â€\n"
        "è¾“å‡º JSONï¼š\n"
        "[\n"
        "  {\"role\": \"æ—ç™½\", \"emotion\": \"æ²‰ç¨³\", \"text\": \"çŒªå…«æˆ’ä¸€è§ï¼ŒæŠŠå˜´ä¸€å™˜ï¼Œå˜Ÿå›”é“ï¼š\", \"bgm\": \"\"},\n"
        "  {\"role\": \"çŒªå…«æˆ’\", \"emotion\": \"å§”å±ˆ\", \"text\": \"å¸ˆçˆ¶ï¼Œç³Ÿç³•äº†ï¼\", \"bgm\": \"funny.mp3\"}\n"
        "]\n\n"
        "è¾“å…¥åŽŸæ–‡ï¼š\n"
        "â€œå¿«èµ°ï¼â€å­™æ‚Ÿç©ºä¸€æŠŠæŽ¨å¼€ä»–ï¼Œâ€œåˆ«ç£¨è¹­ï¼â€\n"
        "è¾“å‡º JSONï¼š\n"
        "[\n"
        "  {\"role\": \"å­™æ‚Ÿç©º\", \"emotion\": \"æ€¥ä¿ƒ\", \"text\": \"å¿«èµ°ï¼\", \"bgm\": \"battle.mp3\"},\n"
        "  {\"role\": \"æ—ç™½\", \"emotion\": \"è®²è¿°æ„Ÿ\", \"text\": \"å­™æ‚Ÿç©ºä¸€æŠŠæŽ¨å¼€ä»–ï¼Œ\", \"bgm\": \"battle.mp3\"},\n"
        "  {\"role\": \"å­™æ‚Ÿç©º\", \"emotion\": \"æ€¥ä¿ƒ\", \"text\": \"åˆ«ç£¨è¹­ï¼\", \"bgm\": \"battle.mp3\"}\n"
        "]\n\n"
        "çŽ°åœ¨ï¼Œè¯·å¤„ç†ä¸‹é¢çš„æ–‡æœ¬ï¼š"
    )
    try:
        completion = client.chat.completions.create(
            model="qwen-max",
            messages=[{"role": "system", "content": system_prompt},{"role": "user", "content": text_content}],
            temperature=0.01 
        )
        return parse_json_output(completion.choices[0].message.content)
    except Exception as e:
        print(f"âŒ LLM é”™è¯¯: {e}")
        return []

# ================= 5. æ ¸å¿ƒæµæ°´çº¿ (å¼‚æ­¥å¹¶å‘ç‰ˆ) =================

class VoiceManager:
    def __init__(self, pool_dir):
        self.pool_dir = pool_dir
        self.all_files = []
        if os.path.exists(pool_dir):
            for root, dirs, files in os.walk(pool_dir):
                for file in files:
                    if file.lower().endswith(('.wav', '.mp3')):
                        self.all_files.append(os.path.join(root, file))
        self.selection_cache = {}

    def _ask_llm_to_pick(self, role_name, emotion):
        if not self.all_files: return None
        file_map = {os.path.basename(f): f for f in self.all_files}
        prompt = f"è§’è‰²: {role_name}, æƒ…ç»ª: {emotion}ã€‚è¯·ä»Žåˆ—è¡¨ {list(file_map.keys())} ä¸­é€‰ä¸€ä¸ªæœ€åˆé€‚çš„æ–‡ä»¶åï¼Œä»…è¾“å‡ºæ–‡ä»¶åï¼Œæ²¡æ‰¾åˆ°è¾“å‡ºNoneã€‚"
        try:
            res = client.chat.completions.create(
                model="qwen-max", messages=[{"role": "user", "content": prompt}], temperature=0.1
            )
            picked = res.choices[0].message.content.strip().replace("'", "").replace('"', "")
            if picked in file_map: return file_map[picked]
        except: pass
        return self.all_files[hash(role_name) % len(self.all_files)]

    def get_smart_voice(self, role_name, emotion=""):
        if role_name in self.selection_cache: return self.selection_cache[role_name]
        selected = self._ask_llm_to_pick(role_name, emotion)
        self.selection_cache[role_name] = selected
        return selected

# --- å•ä¸ªç‰‡æ®µçš„ç”Ÿæˆé€»è¾‘ (å¼‚æ­¥) ---
async def generate_segment_async(index, total, item, user_voice_map, vm, semaphore):
    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
    async with semaphore:
        role = item["è§’è‰²"]
        line = item["å°è¯"]
        raw_emotion = item.get("æƒ…ç»ª", "")  
        bgm_filename = item.get("bgm", "")

        # --- æƒ…ç»ªå®‰å…¨é˜€ ---
        safe_emotion_map = {
            "æ„¤æ€’": "è¯­æ°”å†°å†·", "å’†å“®": "å’¬ç‰™åˆ‡é½¿ï¼Œä½Žæ²‰", "å¤§å–Š": "æ€¥ä¿ƒ",
            "æ­‡æ–¯åº•é‡Œ": "é¢¤æŠ–ï¼Œå“½å’½", "å¤§ç¬‘": "è½»ç¬‘", "ç‹‚ç¬‘": "å¾—æ„çš„ç¬‘",
            "æ‚²ç—›æ¬²ç»": "æ‚²ä¼¤ï¼Œä½Žè½", "ææƒ§": "ç´§å¼ ï¼Œé¢¤éŸ³", "æ¿€æ˜‚": "åšå®šï¼Œæœ‰åŠ›"
        }
        
        if role == "æ—ç™½":
            final_emotion = "æ²‰ç¨³ï¼Œè®²è¿°æ„Ÿï¼Œæ‚¬ç–‘"
        else:
            final_emotion = raw_emotion
            for danger_key, safe_value in safe_emotion_map.items():
                if danger_key in raw_emotion:
                    final_emotion = safe_value
                    break 

        print(f"ðŸ”„ [{index+1}/{total}] è¯·æ±‚ä¸­... {role} ({final_emotion}): {line[:10]}...")

        # --- é€‰è§’é€»è¾‘ ---
        final_wav_path = None
        use_cosy_default = False
        
        if role in user_voice_map: 
            final_wav_path = user_voice_map[role]
        if not final_wav_path:
            for u_role, u_path in user_voice_map.items():
                if u_role != "æ—ç™½" and role != "æ—ç™½" and (u_role in role or role in u_role):
                    final_wav_path = u_path; break
        
        if role == "æ—ç™½":
            if final_wav_path: use_cosy_default = False 
            else: use_cosy_default = True
        
        if not final_wav_path and not use_cosy_default: 
            final_wav_path = vm.get_smart_voice(role, final_emotion)

        # --- å¼‚æ­¥å‘é€ API è¯·æ±‚ ---
        audio_data = None
        async with httpx.AsyncClient(timeout=120.0) as client: 
            try:
                if use_cosy_default:
                    resp = await client.post(URL_COSY, json={"text": line, "speaker": "ä¸­æ–‡å¥³"})
                else:
                    if final_wav_path and os.path.exists(final_wav_path):
                        resp = await client.post(URL_INDEX, json={
                            "text": line, 
                            "emotion": final_emotion, 
                            "ref_audio_path": final_wav_path
                        })
                    else:
                        print(f"   âŒ æ–‡ä»¶ä¸¢å¤±: {final_wav_path}")
                        return None

                if resp.status_code == 200:
                    audio_data = resp.content
                    print(f"   âœ… [{index+1}] ç”Ÿæˆå®Œæ¯•!")
                else:
                    print(f"   âŒ [{index+1}] APIé”™è¯¯: {resp.status_code}")
            except Exception as e:
                print(f"   âŒ [{index+1}] è¯·æ±‚å¼‚å¸¸: {e}")

        return {
            "index": index,
            "audio_data": audio_data,
            "bgm_filename": bgm_filename 
        }

# --- ä¸»æµæ°´çº¿ (å¼‚æ­¥åŒ…è£…) ---
async def process_pipeline_async(task_id: str, text: str, user_voice_map: dict):
    TASKS[task_id]["status"] = "analyzing"
    print("\nðŸ” [1/4] æ­£åœ¨åˆ†æžæ–‡æœ¬...")
    dialogues = analyze_novel_roles_llm(text)
    if not dialogues:
        TASKS[task_id]["status"] = "failed"; return

    vm = VoiceManager(VOICE_POOL_DIR)
    TASKS[task_id]["status"] = "generating"
    TASKS[task_id]["total"] = len(dialogues)

    print(f"\nðŸš€ [2/4] å¯åŠ¨åŒå¡å¹¶å‘ç”Ÿæˆ! (æ€»å¥æ•°: {len(dialogues)})")
    
    semaphore = asyncio.Semaphore(4) 
    
    tasks = []
    for i, item in enumerate(dialogues):
        tasks.append(generate_segment_async(i, len(dialogues), item, user_voice_map, vm, semaphore))
    
    results = await asyncio.gather(*tasks)
    
    results = sorted(results, key=lambda x: x["index"] if x else -1)

    print("\nðŸ”¨ [3/4] æ­£åœ¨åˆå¹¶éŸ³é¢‘å¹¶æ·»åŠ BGM...")
    final_segments = []
    
    for res in results:
        if not res or not res["audio_data"]:
            continue
            
        try:
            import io
            speech_seg = AudioSegment.from_file(io.BytesIO(res["audio_data"]), format="wav")
            speech_seg = match_target_amplitude(speech_seg, -20.0)
            
            bgm_filename = res["bgm_filename"]
            bgm_path = os.path.join(BGM_DIR, bgm_filename) if bgm_filename else None
            
            if bgm_path and os.path.exists(bgm_path):
                mixed_seg = mix_speech_with_bgm(speech_seg, bgm_path)
            else:
                mixed_seg = speech_seg
                
            final_segments.append(mixed_seg)
            final_segments.append(AudioSegment.silent(duration=300))
            
            TASKS[task_id]["progress"] = int((res["index"] / len(dialogues)) * 100)
            
        except Exception as e:
            print(f"åˆå¹¶å‡ºé”™: {e}")

    if not final_segments:
        TASKS[task_id]["status"] = "failed"; return

    full_audio = AudioSegment.empty()
    for seg in final_segments:
        full_audio += seg

    final_name = f"{task_id}.mp3"
    full_audio.export(os.path.join(OUTPUT_DIR, final_name), format="mp3")
    
    TASKS[task_id]["status"] = "completed"
    TASKS[task_id]["result_url"] = f"/download/{final_name}"
    TASKS[task_id]["progress"] = 100
    print(f"\nðŸŽ‰ [4/4] ä»»åŠ¡å®Œæˆï¼Œæ–‡ä»¶: {final_name}\n")

def run_async_pipeline(task_id, text, user_voice_map):
    asyncio.run(process_pipeline_async(task_id, text, user_voice_map))

# ================= 6. API æŽ¥å£ =================

@app.post("/analyze")
async def analyze_endpoint(file: UploadFile = File(...)):
    content = await file.read()
    text = content.decode("utf-8")
    dialogues = analyze_novel_roles_llm(text)
    unique_roles = set(item['è§’è‰²'] for item in dialogues)
    return {"roles": sorted(list(unique_roles), key=lambda x: 0 if x == "æ—ç™½" else 1)}

@app.post("/generate_step")
async def generate_step(request: Request, bg_tasks: BackgroundTasks):
    form = await request.form()
    file = form.get("file")
    if not file: return JSONResponse(400, {"error": "No file"})
    content = await file.read()
    text = content.decode("utf-8")
    
    user_voice_map = {}
    print("\nðŸ” [DEBUG] æŽ¥æ”¶å‰ç«¯è¡¨å•æ•°æ®:")
    for k, v in form.items():
        if k == "file": continue
        if k.startswith("custom_voice_") and hasattr(v, "filename") and v.filename:
            role = k.replace("custom_voice_", "")
            safe_name = f"{uuid.uuid4()}_{v.filename}"
            save_path = os.path.join(TEMP_VOICE_DIR, safe_name)
            with open(save_path, "wb") as f: shutil.copyfileobj(v.file, f)
            user_voice_map[role] = os.path.abspath(save_path)
            print(f"   ðŸ“‚ æ”¶åˆ°æ–‡ä»¶: [{role}] -> {v.filename}")
        elif k.startswith("preset_voice_") and isinstance(v, str) and v:
            role = k.replace("preset_voice_", "")
            path = os.path.join(VOICE_POOL_DIR, v)
            if os.path.exists(path):
                user_voice_map[role] = os.path.abspath(path)
                print(f"   ðŸŽµ æ”¶åˆ°é¢„è®¾: [{role}] -> {v}")

    task_id = str(uuid.uuid4())
    TASKS[task_id] = {"status": "pending", "progress": 0}
    bg_tasks.add_task(run_async_pipeline, task_id, text, user_voice_map)
    return {"task_id": task_id}

@app.get("/status/{task_id}")
def status(task_id: str): return TASKS.get(task_id, {})

@app.get("/download/{name}")
def download(name: str):
    path = os.path.join(OUTPUT_DIR, name)
    return FileResponse(path) if os.path.exists(path) else JSONResponse(404)

@app.get("/", response_class=HTMLResponse)
async def read_root():
    if os.path.exists("index.html"):
        with open("index.html", "r", encoding="utf-8") as f: return f.read()
    return "<h1>index.html Not Found</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8039)