import os
import json
import uuid
import requests
import re
import shutil
import time
import glob
import random
import traceback
from fastapi import FastAPI, UploadFile, File, BackgroundTasks, Request, HTTPException
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydub import AudioSegment

from openai import OpenAI

app = FastAPI()
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

# ================= 1. é…ç½®è·¯å¾„ =================
UPLOAD_DIR = "uploads"
OUTPUT_DIR = "outputs"
TEMP_VOICE_DIR = "uploads/custom_voices"
VOICE_POOL_DIR = "/home/nyw/AI-practice/resource/input_audio"
BGM_DIR = "/home/nyw/AI-practice/resource/pre_train_wav/background"  # BGM åº“

# ç¡®ä¿ç›®å½•å­˜åœ¨
for d in [UPLOAD_DIR, OUTPUT_DIR, TEMP_VOICE_DIR, VOICE_POOL_DIR, BGM_DIR]:
    os.makedirs(d, exist_ok=True)

# GPU æœåŠ¡åœ°å€
URL_COSY = "http://localhost:8005/generate" # æ³¨æ„ä½ ä¹‹å‰çš„é…ç½®ç«¯å£
URL_INDEX = "http://localhost:8002/generate"

TASKS = {}

# ================= 2. åˆå§‹åŒ–å¤§æ¨¡å‹ =================
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY") or "sk-cfc644272f8b4be2aa58f9b240636083"
client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# ================= 3. å·¥å…·å‡½æ•°ï¼šéŸ³é¢‘å¤„ç† =================

def match_target_amplitude(sound, target_dBFS=-20.0):
    """å°†éŸ³é¢‘å“åº¦ç»Ÿä¸€è°ƒæ•´åˆ° target_dBFS"""
    change_in_dBFS = target_dBFS - sound.dBFS
    return sound.apply_gain(change_in_dBFS)

def mix_speech_with_bgm(speech_seg, bgm_path):
    """
    å•å¥æ··åˆé€»è¾‘ï¼š
    1. è¯»å– BGM
    2. å¾ªç¯ BGM ç›´åˆ°è¦†ç›–äººå£°é•¿åº¦
    3. è°ƒæ•´ BGM éŸ³é‡ï¼ˆå‹ä½ï¼‰
    4. è£å‰ªå¹¶åšæ·¡å…¥æ·¡å‡º
    5. æ··åˆ
    """
    if not bgm_path or not os.path.exists(bgm_path):
        return speech_seg # æ²¡æœ‰BGMåˆ™åŸæ ·è¿”å›
    
    try:
        bgm = AudioSegment.from_file(bgm_path)
        
        # 1. ç»Ÿä¸€åŸºå‡†éŸ³é‡
        bgm = match_target_amplitude(bgm, -20.0)
        
        # 2. å‹ä½èƒŒæ™¯éŸ³ (æ¯”äººå£°ä½ 12dBï¼Œä¿è¯äººå£°æ¸…æ™°)
        bgm = bgm - 12 
        
        # 3. å¾ªç¯å¡«å……ï¼šå¦‚æœ BGM çŸ­äºäººå£°ï¼Œè¿›è¡Œå¾ªç¯
        # é¢å¤–åŠ  500ms å°¾éŸµï¼Œé˜²æ­¢æˆªæ–­å¤ªç”Ÿç¡¬
        target_len = len(speech_seg) + 500
        if len(bgm) < target_len:
            loop_count = (target_len // len(bgm)) + 1
            bgm = bgm * loop_count
            
        # 4. ç²¾ç¡®è£å‰ª
        bgm = bgm[:target_len]
        
        # 5. æ·¡å…¥æ·¡å‡º (é˜²æ­¢ä¸åŒBGMåˆ‡æ¢æ—¶çš„çˆ†éŸ³)
        # å¼€å¤´æ·¡å…¥ 500msï¼Œç»“å°¾æ·¡å‡º 500ms
        bgm = bgm.fade_in(500).fade_out(500)
        
        # 6. å åŠ ï¼šBGM å¯èƒ½ä¼šæ¯”äººå£°é•¿ä¸€ç‚¹ç‚¹ï¼ˆå°¾éŸµï¼‰ï¼Œoverlay ä¼šè‡ªåŠ¨æ‰©å±•é•¿åº¦
        # position=0 è¡¨ç¤ºä»å¤´å¼€å§‹å 
        mixed = speech_seg.overlay(bgm, position=0)
        return mixed

    except Exception as e:
        print(f"âš ï¸ BGMèåˆå¤±è´¥ [{os.path.basename(bgm_path)}]: {e}")
        return speech_seg

# ================= 4. LLM åˆ†æé€»è¾‘ (å‡çº§ç‰ˆ) =================

def get_all_bgm_filenames():
    """è·å– BGM ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶å"""
    files = []
    if os.path.exists(BGM_DIR):
        for f in os.listdir(BGM_DIR):
            if f.lower().endswith(('.mp3', '.wav', '.flac')):
                files.append(f)
    return files

def parse_json_output(text_output):
    print(f"----- LLM åŸå§‹è¿”å› (å‰100å­—) -----\n{text_output[:100]}...\n-------------------------------")
    clean_text = re.sub(r'```json\s*', '', text_output)
    clean_text = re.sub(r'```', '', clean_text).strip()
    try:
        data = json.loads(clean_text)
        results = []
        for item in data:
            role = item.get("role", item.get("è§’è‰²", "æ—ç™½")).strip()
            emotion = item.get("emotion", item.get("æƒ…ç»ª", "å¹³æ·¡"))
            text = item.get("text", item.get("å°è¯", ""))
            bgm = item.get("bgm", "") # è·å– BGM å­—æ®µ
            
            # å¼ºåˆ¶ç»Ÿä¸€æ—ç™½
            if "æ—" in role and "ç™½" in role: role = "æ—ç™½"
            if role.lower() == "narrator": role = "æ—ç™½"
            
            results.append({"è§’è‰²": role, "æƒ…ç»ª": emotion, "å°è¯": text, "bgm": bgm})
        return results
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {e}")

def analyze_novel_roles_llm(text_content):
    # 1. è·å–æ‰€æœ‰å¯ç”¨çš„ BGM æ–‡ä»¶å
    bgm_files = get_all_bgm_filenames()
    bgm_list_str = json.dumps(bgm_files, ensure_ascii=False)
    
    # 2. æ„å»º Prompt
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæœ‰å£°ä¹¦å¯¼æ¼”ã€‚è¯·å°†æ–‡æœ¬æ‹†è§£ä¸º JSON æ•°ç»„ã€‚\n"
        f"å¯ç”¨çš„èƒŒæ™¯éŸ³ä¹/éŸ³æ•ˆåº“å¦‚ä¸‹ï¼š{bgm_list_str}\n\n"
        "è¦æ±‚ï¼š\n"
        "1. å­—æ®µåŒ…æ‹¬ï¼šrole (è§’è‰²), emotion (æƒ…ç»ª), text (å°è¯), bgm (ä»ä¸Šè¿°åˆ—è¡¨ä¸­é€‰ä¸€ä¸ªæœ€åŒ¹é…çš„æ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰åˆé€‚çš„æˆ–ä¸éœ€è¦ï¼Œå¡«ç©ºå­—ç¬¦ä¸²)ã€‚\n"
        "2. è§’è‰²åå¿…é¡»ç»Ÿä¸€ã€‚\n"
        "3. æ‰€æœ‰æ—ç™½çš„è§’è‰²åå…¨éƒ¨ç»Ÿä¸€ä¸ºâ€œæ—ç™½â€ã€‚\n"
        "4. ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ã€‚\n"
        "5. ã€é‡è¦ã€‘ä¸ºäº†ä¿è¯é…éŸ³ç¨³å®šï¼Œemotion (æƒ…ç»ª) å­—æ®µå¿…é¡»ä¿æŒå…‹åˆ¶ã€‚å³ä½¿åŸæ–‡æå†™éå¸¸æ¿€çƒˆï¼ˆå¦‚æ­‡æ–¯åº•é‡Œã€å’†å“®ã€å¤§å“­ï¼‰ï¼Œä¹Ÿè¯·è½¬åŒ–ä¸ºç›¸å¯¹æ”¶æ•›çš„æè¿°ï¼Œä¾‹å¦‚ 'å‹æŠ‘çš„æ„¤æ€’'ã€'å†·å³»'ã€'æ€¥ä¿ƒ'ã€'ä½æ²‰'ã€'å“½å’½' ç­‰ã€‚ç»å¯¹é¿å…ä½¿ç”¨ä¼šå¯¼è‡´å£°éŸ³å¤±çœŸçš„æç«¯æƒ…ç»ªè¯ã€‚"
    )

    try:
        completion = client.chat.completions.create(
            model="qwen-max",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text_content}
            ],
            temperature=0.1 
        )
        return parse_json_output(completion.choices[0].message.content)
    except Exception as e:
        print(f"âŒ LLM é”™è¯¯: {e}")
        return []

# ================= 5. æ ¸å¿ƒæµæ°´çº¿ (å•å¥èåˆç‰ˆ) =================

class VoiceManager:
    def __init__(self, pool_dir):
        self.pool_dir = pool_dir
        self.all_files = []
        if os.path.exists(pool_dir):
            for root, dirs, files in os.walk(pool_dir):
                for file in files:
                    if file.lower().endswith(('.wav', '.mp3')):
                        self.all_files.append(os.path.join(root, file))
        self.selection_cache = {}

    def _ask_llm_to_pick(self, role_name, emotion):
        if not self.all_files: return None
        file_map = {os.path.basename(f): f for f in self.all_files}
        prompt = f"è§’è‰²: {role_name}, æƒ…ç»ª: {emotion}ã€‚è¯·ä»åˆ—è¡¨ {list(file_map.keys())} ä¸­é€‰ä¸€ä¸ªæœ€åˆé€‚çš„æ–‡ä»¶åï¼Œä»…è¾“å‡ºæ–‡ä»¶åï¼Œæ²¡æ‰¾åˆ°è¾“å‡ºNoneã€‚"
        try:
            res = client.chat.completions.create(
                model="qwen-max", messages=[{"role": "user", "content": prompt}], temperature=0.1
            )
            picked = res.choices[0].message.content.strip().replace("'", "").replace('"', "")
            if picked in file_map: return file_map[picked]
        except: pass
        return self.all_files[hash(role_name) % len(self.all_files)]

    def get_smart_voice(self, role_name, emotion=""):
        if role_name in self.selection_cache: return self.selection_cache[role_name]
        selected = self._ask_llm_to_pick(role_name, emotion)
        self.selection_cache[role_name] = selected
        return selected
# ================= 5. æ ¸å¿ƒæµæ°´çº¿ (å¸¦éŸ³è‰²æ—¥å¿—ç‰ˆ) =================

def process_pipeline_v2(task_id: str, text: str, user_voice_map: dict):
    TASKS[task_id]["status"] = "analyzing"
    
    # --- 1. è§’è‰²ä¸BGMåˆ†æ ---
    print("\nğŸ” [1/4] æ­£åœ¨åˆ†ææ–‡æœ¬å¹¶åˆ†é…BGM...")
    dialogues = analyze_novel_roles_llm(text)
    if not dialogues:
        TASKS[task_id]["status"] = "failed"; return

    vm = VoiceManager(VOICE_POOL_DIR)
    
    TASKS[task_id]["status"] = "generating"
    TASKS[task_id]["total"] = len(dialogues)
    
    final_segments = []

    # --- 2. é€å¥ç”Ÿæˆ + å®æ—¶èåˆ ---
    print("\nğŸ—£ï¸ [2/4] å¼€å§‹ç”Ÿæˆè¯­éŸ³å¹¶èåˆèƒŒæ™¯éŸ³...")
    for i, item in enumerate(dialogues):
        TASKS[task_id]["progress"] = int((i / len(dialogues)) * 100)
        role = item["è§’è‰²"]
        line = item["å°è¯"]
        emotion = item.get("æƒ…ç»ª", "")
        bgm_filename = item.get("bgm", "")
        
        # æ‰“å°å½“å‰å¥å­çš„åŸºæœ¬ä¿¡æ¯
        bgm_info = f"ğŸµ {bgm_filename}" if bgm_filename else "æ— BGM"
        print(f"\nâ¡ï¸ [{i+1}/{len(dialogues)}] {role}: {line[:15]}... | {bgm_info}")

        try:
            # === A. ç¡®å®šéŸ³è‰²é€»è¾‘ ===
            final_wav_path = None
            use_cosy_default = False
            voice_source_type = "æœªçŸ¥"

            # 1. å°è¯•ç”¨æˆ·æŒ‡å®š (ç²¾ç¡®åŒ¹é…)
            if role in user_voice_map: 
                final_wav_path = user_voice_map[role]
                voice_source_type = "ç”¨æˆ·é”å®š"
            
            # 2. å°è¯•ç”¨æˆ·æŒ‡å®š (æ¨¡ç³ŠåŒ¹é…)
            if not final_wav_path:
                for u_role, u_path in user_voice_map.items():
                    if u_role != "æ—ç™½" and role != "æ—ç™½" and (u_role in role or role in u_role):
                        final_wav_path = u_path
                        voice_source_type = f"ç”¨æˆ·æ¨¡ç³Š({u_role})"
                        break
            
            # 3. æ—ç™½é»˜è®¤é€»è¾‘
            if not final_wav_path and role == "æ—ç™½": 
                use_cosy_default = True
                voice_source_type = "ç³»ç»Ÿé»˜è®¤"

            # 4. AI è‡ªåŠ¨é€‰è§’
            if not final_wav_path and not use_cosy_default: 
                final_wav_path = vm.get_smart_voice(role, emotion)
                voice_source_type = "AIè‡ªåŠ¨"

            # === B. æ‰“å°éŸ³è‰²é€‰æ‹©æ—¥å¿— (è¿™æ˜¯ä½ æƒ³è¦çš„åŠŸèƒ½) ===
            if use_cosy_default:
                print(f"   ğŸ™ï¸ [éŸ³è‰²] {voice_source_type} -> CosyVoice (ä¸­æ–‡å¥³)")
            elif final_wav_path:
                print(f"   ğŸ™ï¸ [éŸ³è‰²] {voice_source_type} -> æ–‡ä»¶: {os.path.basename(final_wav_path)}")
            else:
                print(f"   âš ï¸ [éŸ³è‰²] æœªæ‰¾åˆ°å¯ç”¨éŸ³è‰²ï¼Œå°†è·³è¿‡ç”Ÿæˆï¼")
                continue

            # === C. å‘é€ç”Ÿæˆè¯·æ±‚ ===
            audio_data = None
            
            if use_cosy_default:
                resp = requests.post(URL_COSY, json={"text": line, "speaker": "ä¸­æ–‡å¥³"}, timeout=60)
            else:
                if final_wav_path and os.path.exists(final_wav_path):
                    resp = requests.post(URL_INDEX, json={"text": line, "emotion": emotion, "ref_audio_path": final_wav_path}, timeout=60)
                else:
                    print(f"   âš ï¸ å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸¢å¤±: {final_wav_path}")
                    continue

            if resp and resp.status_code == 200:
                audio_data = resp.content
            else:
                print(f"   âŒ ç”ŸæˆAPIæŠ¥é”™: Code {resp.status_code if resp else 'None'}")
                continue

            # === D. éŸ³é¢‘åå¤„ç† (å½’ä¸€åŒ– & BGM) ===
            import io
            speech_seg = AudioSegment.from_file(io.BytesIO(audio_data), format="wav")
            speech_seg = match_target_amplitude(speech_seg, -20.0)
            
            # èåˆ BGM
            bgm_path = os.path.join(BGM_DIR, bgm_filename) if bgm_filename else None
            if bgm_path and os.path.exists(bgm_path):
                mixed_seg = mix_speech_with_bgm(speech_seg, bgm_path)
            else:
                mixed_seg = speech_seg

            final_segments.append(mixed_seg)
            final_segments.append(AudioSegment.silent(duration=300))

        except Exception as e:
            print(f"   âŒ å¤„ç†å¼‚å¸¸: {e}")
            traceback.print_exc()

    # --- 3. æœ€ç»ˆåˆå¹¶ ---
    if not final_segments:
        TASKS[task_id]["status"] = "failed"; return

    print("\nğŸ”¨ [3/4] æ­£åœ¨å¯¼å‡ºæœ€ç»ˆæ–‡ä»¶...")
    full_audio = AudioSegment.empty()
    for seg in final_segments:
        full_audio += seg

    final_name = f"{task_id}.mp3"
    full_audio.export(os.path.join(OUTPUT_DIR, final_name), format="mp3")
    
    TASKS[task_id]["status"] = "completed"
    TASKS[task_id]["result_url"] = f"/download/{final_name}"
    TASKS[task_id]["progress"] = 100
    print(f"\nğŸ‰ [4/4] ä»»åŠ¡å®Œæˆï¼Œæ–‡ä»¶: {final_name}\n")

# ================= 6. API æ¥å£ =================

@app.post("/analyze")
async def analyze_endpoint(file: UploadFile = File(...)):
    content = await file.read()
    text = content.decode("utf-8")
    dialogues = analyze_novel_roles_llm(text)
    unique_roles = set(item['è§’è‰²'] for item in dialogues)
    return {"roles": sorted(list(unique_roles), key=lambda x: 0 if x == "æ—ç™½" else 1)}

@app.post("/generate_step")
async def generate_step(request: Request, bg_tasks: BackgroundTasks):
    form = await request.form()
    file = form.get("file")
    if not file: return JSONResponse(400, {"error": "No file"})
    content = await file.read()
    text = content.decode("utf-8")
    
    user_voice_map = {}
    print("\nğŸ” [DEBUG] æ¥æ”¶å‰ç«¯è¡¨å•æ•°æ®:")
    for k, v in form.items():
        if k == "file": continue
        if k.startswith("custom_voice_") and hasattr(v, "filename") and v.filename:
            role = k.replace("custom_voice_", "")
            safe_name = f"{uuid.uuid4()}_{v.filename}"
            save_path = os.path.join(TEMP_VOICE_DIR, safe_name)
            with open(save_path, "wb") as f: shutil.copyfileobj(v.file, f)
            user_voice_map[role] = os.path.abspath(save_path)
            
        elif k.startswith("preset_voice_") and isinstance(v, str) and v:
            role = k.replace("preset_voice_", "")
            path = os.path.join(VOICE_POOL_DIR, v)
            if os.path.exists(path):
                user_voice_map[role] = os.path.abspath(path)

    task_id = str(uuid.uuid4())
    TASKS[task_id] = {"status": "pending", "progress": 0}
    bg_tasks.add_task(process_pipeline_v2, task_id, text, user_voice_map)
    return {"task_id": task_id}

@app.get("/status/{task_id}")
def status(task_id: str): return TASKS.get(task_id, {})

@app.get("/download/{name}")
def download(name: str):
    path = os.path.join(OUTPUT_DIR, name)
    return FileResponse(path) if os.path.exists(path) else JSONResponse(404)

@app.get("/", response_class=HTMLResponse)
async def read_root():
    if os.path.exists("index.html"):
        with open("index.html", "r", encoding="utf-8") as f: return f.read()
    return "<h1>index.html Not Found</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)