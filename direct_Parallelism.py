import os
import json
import uuid
import requests
import re
import shutil
import time
import glob
import random
import traceback
import asyncio
import httpx
from fastapi import FastAPI, UploadFile, File, BackgroundTasks, Request, HTTPException
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydub import AudioSegment

from openai import OpenAI

app = FastAPI()
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

# ================= 1. é…ç½®è·¯å¾„ =================
UPLOAD_DIR = "uploads"
OUTPUT_DIR = "outputs"
TEMP_VOICE_DIR = "uploads/custom_voices"
VOICE_POOL_DIR = "/home/nyw/AI-practice/resource/input_audio"
BGM_DIR = "/home/nyw/AI-practice/resource/pre_train_wav/background" 

for d in [UPLOAD_DIR, OUTPUT_DIR, TEMP_VOICE_DIR, VOICE_POOL_DIR, BGM_DIR]:
    os.makedirs(d, exist_ok=True)

# GPU æœåŠ¡åœ°å€
URL_COSY = "http://localhost:8005/generate" 
URL_INDEX = "http://localhost:8002/generate"

TASKS = {}

# ================= 2. åˆå§‹åŒ–å¤§æ¨¡å‹ =================
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY") or "sk-cfc644272f8b4be2aa58f9b240636083"
client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# ================= 3. å·¥å…·å‡½æ•° =================

def match_target_amplitude(sound, target_dBFS=-20.0):
    change_in_dBFS = target_dBFS - sound.dBFS
    return sound.apply_gain(change_in_dBFS)

def mix_speech_with_bgm(speech_seg, bgm_path):
    if not bgm_path or not os.path.exists(bgm_path):
        return speech_seg 
    try:
        bgm = AudioSegment.from_file(bgm_path)
        bgm = match_target_amplitude(bgm, -20.0)
        bgm = bgm - 12 
        target_len = len(speech_seg) + 500
        if len(bgm) > target_len:
            bgm = bgm[:target_len]
            bgm = bgm.fade_out(500)
        bgm = bgm.fade_in(500)
        mixed = speech_seg.overlay(bgm, position=0)
        return mixed
    except Exception as e:
        print(f"âš ï¸ BGMèåˆå¤±è´¥ [{os.path.basename(bgm_path)}]: {e}")
        return speech_seg

# ================= 4. LLM åˆ†æé€»è¾‘ =================

def get_all_bgm_filenames():
    files = []
    if os.path.exists(BGM_DIR):
        for f in os.listdir(BGM_DIR):
            if f.lower().endswith(('.mp3', '.wav', '.flac')):
                files.append(f)
    return files

def parse_json_output(text_output):
    print(f"----- LLM åŸå§‹è¿”å› (å‰100å­—) -----\n{text_output[:100]}...\n-------------------------------")
    clean_text = re.sub(r'```json\s*', '', text_output)
    clean_text = re.sub(r'```', '', clean_text).strip()
    try:
        data = json.loads(clean_text)
        results = []
        for item in data:
            role = item.get("role", item.get("è§’è‰²", "æ—ç™½")).strip()
            emotion = item.get("emotion", item.get("æƒ…ç»ª", "å¹³æ·¡"))
            text = item.get("text", item.get("å°è¯", ""))
            bgm = item.get("bgm", "") 
            if "æ—" in role and "ç™½" in role: role = "æ—ç™½"
            if role.lower() == "narrator": role = "æ—ç™½"
            results.append({"è§’è‰²": role, "æƒ…ç»ª": emotion, "å°è¯": text, "bgm": bgm})
        return results
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {e}")
        return []

def analyze_novel_roles_llm(text_content):
    bgm_files = get_all_bgm_filenames()
    bgm_list_str = json.dumps(bgm_files, ensure_ascii=False)
    
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœ‰å£°ä¹¦è„šæœ¬ç¼–è¾‘ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†å°è¯´åŸæ–‡æ‹†è§£ä¸ºè¯­éŸ³åˆæˆï¼ˆTTSï¼‰æ‰€éœ€çš„ JSON æ ¼å¼ã€‚\n"
        f"ã€BGMç´ æåº“ã€‘ï¼š{bgm_list_str}\n\n"
        "ã€æ ¸å¿ƒåŸåˆ™ï¼ˆå¿…é¡»æ­»å®ˆï¼‰ã€‘\n"
        "1. **åŸæ–‡è¿˜åŸ**ï¼šä¸¥ç¦ä¿®æ”¹ã€åˆ å‡ã€å¢åŠ åŸæ–‡çš„ä»»ä½•ä¸€ä¸ªå­—æˆ–æ ‡ç‚¹ç¬¦å·ï¼\n"
        "2. **ç•Œé™åˆ†æ˜**ï¼š\n"
        "   - å¼•å· `â€œ...â€` å†…éƒ¨çš„å†…å®¹ -> å½’å±å¯¹åº”è§’è‰²ã€‚\n"
        "   - å¼•å·å¤–éƒ¨çš„å†…å®¹ï¼ˆåŒ…æ‹¬â€˜é“â€™ã€â€˜è¯´â€™ã€åŠ¨ä½œã€å¿ƒç†ã€ç¯å¢ƒï¼‰ -> å…¨éƒ¨å½’å±è§’è‰² 'æ—ç™½'ã€‚\n"
        "3. **å¿…é¡»åˆ‡åˆ†**ï¼šé‡åˆ° [å¯¹è¯] + [æå†™] + [å¯¹è¯] çš„ç»“æ„ï¼Œå¿…é¡»æ‹†åˆ†æˆ 3 ä¸ªç‹¬ç«‹çš„ JSON å¯¹è±¡ï¼Œç»å¯¹ä¸èƒ½åˆå¹¶ï¼\n"
        "4. **æƒ…ç»ªé™çº§**ï¼šemotion å­—æ®µå¿…é¡»ä½¿ç”¨ä¹¦é¢è¯­ä¸”å…‹åˆ¶ï¼Œé˜²æ­¢TTSçˆ†éŸ³ã€‚\n"
        "5. **æ—ç™½äººè®¾**ï¼šæ—ç™½æ°¸è¿œæ˜¯å†·é™çš„.\n"
        "6. **BGMè§„åˆ™**ï¼šä»…å½“æ–‡æœ¬æ˜æ˜¾ä½“ç°å‡ºç´ æåº“ä¸­æŸä¸ªæ–‡ä»¶åçš„æ°›å›´æ—¶ï¼ˆå¦‚å†™äº†'é›¨'ä¸”åº“é‡Œæœ‰'rain'ï¼‰ï¼Œæ‰å¡«å…¥bgmå­—æ®µï¼Œå¦åˆ™å¡«ç©ºå­—ç¬¦ä¸² \"\"ã€‚\n\n"
        "ã€æ‹†è§£ç¤ºä¾‹ï¼ˆè¯·ä¸¥æ ¼æ¨¡ä»¿ï¼‰ã€‘\n"
        "è¾“å…¥ï¼š\n"
        "æå››ä¸€æ‹æ¡Œå­ï¼Œæ€’é“ï¼šâ€œä½ æ•¢ï¼â€è¯´ç€ä¾¿å†²äº†ä¸Šå»ã€‚\n"
        "è¾“å‡ºï¼š\n"
        "[\n"
        "  {\"role\": \"æ—ç™½\", \"emotion\": \"æ²‰ç¨³\", \"text\": \"æå››ä¸€æ‹æ¡Œå­ï¼Œæ€’é“ï¼š\", \"bgm\": \"\"},\n"
        "  {\"role\": \"æå››\", \"emotion\": \"æ„¤æ€’ã€åŒæ¶\", \"text\": \"â€œä½ æ•¢ï¼â€\", \"bgm\": \"tension.mp3\"},\n"
        "  {\"role\": \"æ—ç™½\", \"emotion\": \"æ€¥ä¿ƒ\", \"text\": \"è¯´ç€ä¾¿å†²äº†ä¸Šå»ã€‚\", \"bgm\": \"tension.mp3\"}\n"
        "]\n\n"
        "è¾“å…¥ï¼š\n"
        "â€œåœ¨å®¶ç¡è§‰ï¼Ÿâ€æå³°å†·ç¬‘äº†ä¸€å£°ï¼ŒçŒ›åœ°å°†ä¸€ä»½æ–‡ä»¶æ‘”åœ¨æ¡Œå­ä¸Šï¼Œå“å¾—ç‹å¼ºçŒ›åœ°ä¸€ç¼©è„–å­ã€‚"
        "è¾“å‡ºï¼š\n"
        "[\n"
        "  {\"role\": \"æå³°\", \"emotion\": \"åŒæ¶\", \"text\": \"åœ¨å®¶ç¡è§‰ï¼Ÿ\", \"bgm\": \"\"},\n"
        "  {\"role\": \"æ—ç™½\", \"emotion\": \"è‡ªç„¶ã€æ²‰ç¨³\", \"text\": \"æå³°å†·ç¬‘äº†ä¸€å£°ï¼ŒçŒ›åœ°å°†ä¸€ä»½æ–‡ä»¶æ‘”åœ¨æ¡Œå­ä¸Šï¼Œå“å¾—ç‹å¼ºçŒ›åœ°ä¸€ç¼©è„–å­\", \"bgm\": \"\"},\n"
        "]\n\n"

        "ç°åœ¨ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„åˆ™å¤„ç†ä»¥ä¸‹æ–‡æœ¬ï¼Œç›´æ¥è¾“å‡º JSON æ•°ç»„ï¼š"
    )

    try:
        completion = client.chat.completions.create(
            model="qwen-max",
            messages=[{"role": "system", "content": system_prompt},{"role": "user", "content": text_content}],
            temperature=0.01 
        )
        return parse_json_output(completion.choices[0].message.content)
    except Exception as e:
        print(f"âŒ LLM é”™è¯¯: {e}")
        return []

# ================= 5. æ ¸å¿ƒæµæ°´çº¿ (å¼‚æ­¥å¹¶å‘ç‰ˆ) =================

class VoiceManager:
    def __init__(self, pool_dir):
        self.pool_dir = pool_dir
        self.all_files = []
        if os.path.exists(pool_dir):
            for root, dirs, files in os.walk(pool_dir):
                for file in files:
                    if file.lower().endswith(('.wav', '.mp3')):
                        self.all_files.append(os.path.join(root, file))
        self.selection_cache = {}

    def _ask_llm_to_pick(self, role_name, emotion):
        if not self.all_files: return None
        file_map = {os.path.basename(f): f for f in self.all_files}
        prompt = f"è§’è‰²: {role_name}, æƒ…ç»ª: {emotion}ã€‚è¯·ä»åˆ—è¡¨ {list(file_map.keys())} ä¸­é€‰ä¸€ä¸ªæœ€åˆé€‚çš„æ–‡ä»¶åï¼Œä»…è¾“å‡ºæ–‡ä»¶åï¼Œæ²¡æ‰¾åˆ°è¾“å‡ºNoneã€‚"
        try:
            res = client.chat.completions.create(
                model="qwen-max", messages=[{"role": "user", "content": prompt}], temperature=0.1
            )
            picked = res.choices[0].message.content.strip().replace("'", "").replace('"', "")
            if picked in file_map: return file_map[picked]
        except: pass
        return self.all_files[hash(role_name) % len(self.all_files)]

    def get_smart_voice(self, role_name, emotion=""):
        if role_name in self.selection_cache: return self.selection_cache[role_name]
        selected = self._ask_llm_to_pick(role_name, emotion)
        self.selection_cache[role_name] = selected
        return selected

# --- å•ä¸ªç‰‡æ®µçš„ç”Ÿæˆé€»è¾‘ (å¼‚æ­¥) ---
async def generate_segment_async(index, total, item, user_voice_map, vm, semaphore):
    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
    async with semaphore:
        role = item["è§’è‰²"]
        line = item["å°è¯"]
        raw_emotion = item.get("æƒ…ç»ª", "")  
        bgm_filename = item.get("bgm", "")

        #--- æƒ…ç»ªå®‰å…¨é˜€ ---
        safe_emotion_map = {
            "æ„¤æ€’": "è¯­æ°”å†°å†·", "å’†å“®": "å’¬ç‰™åˆ‡é½¿ï¼Œä½æ²‰", "å¤§å–Š": "æ€¥ä¿ƒ",
            "æ­‡æ–¯åº•é‡Œ": "é¢¤æŠ–ï¼Œå“½å’½", "å¤§ç¬‘": "è½»ç¬‘", "ç‹‚ç¬‘": "å¾—æ„çš„ç¬‘",
            "æ‚²ç—›æ¬²ç»": "æ‚²ä¼¤ï¼Œä½è½", "ææƒ§": "ç´§å¼ ï¼Œé¢¤éŸ³", "æ¿€æ˜‚": "åšå®šï¼Œæœ‰åŠ›"
        }
        
        if role == "æ—ç™½":
            final_emotion = "è®²è¿°æ„Ÿï¼Œè‡ªç„¶"
        else:
            final_emotion = raw_emotion
            for danger_key, safe_value in safe_emotion_map.items():
                if danger_key in raw_emotion:
                    final_emotion = safe_value
                    break 

        # --- é€‰è§’é€»è¾‘ ---
        final_wav_path = None
        use_cosy_default = False
        
        # 1. æŸ¥ç”¨æˆ·è¡¨
        if role in user_voice_map: 
            final_wav_path = user_voice_map[role]
        if not final_wav_path:
            for u_role, u_path in user_voice_map.items():
                if u_role != "æ—ç™½" and role != "æ—ç™½" and (u_role in role or role in u_role):
                    final_wav_path = u_path; break
        
        # 2. æ—ç™½ç‰¹æ®Šå¤„ç†
        if role == "æ—ç™½":
            if final_wav_path: use_cosy_default = False 
            else: use_cosy_default = True
        
        # 3. AI è‡ªåŠ¨é€‰è§’
        if not final_wav_path and not use_cosy_default: 
            final_wav_path = vm.get_smart_voice(role, final_emotion)

        # === ğŸ“‹ è¯¦ç»†çš„æ§åˆ¶å°æ—¥å¿—æ‰“å° (ç”¨æˆ·æ ¸å¿ƒéœ€æ±‚) ===
        voice_log = "æœªçŸ¥"
        if use_cosy_default:
            voice_log = "CosyVoice (é»˜è®¤å¥³å£°)"
        elif final_wav_path:
            voice_log = f"{os.path.basename(final_wav_path)}"
        else:
            voice_log = "âš ï¸ æœªæ‰¾åˆ°å¯ç”¨éŸ³è‰²"

        bgm_log = bgm_filename if bgm_filename else "ğŸˆš"

        print(f"\nâ¡ï¸ [{index+1}/{total}] {role}: {line[:15]}...")
        print(f"   ğŸ™ï¸ éŸ³è‰²: {voice_log}")
        print(f"   ğŸ­ æƒ…ç»ª: {final_emotion}")
        print(f"   ğŸµ BGM : {bgm_log}")
        # ==========================================

        # --- å¼‚æ­¥å‘é€ API è¯·æ±‚ ---
        audio_data = None
        async with httpx.AsyncClient(timeout=120.0) as client: 
            try:
                if use_cosy_default:
                    resp = await client.post(URL_COSY, json={"text": line, "speaker": "ä¸­æ–‡å¥³"})
                else:
                    if final_wav_path and os.path.exists(final_wav_path):
                        resp = await client.post(URL_INDEX, json={
                            "text": line, 
                            "emotion": final_emotion, 
                            "ref_audio_path": final_wav_path
                        })
                    else:
                        print(f"   âŒ æ–‡ä»¶ä¸¢å¤±: {final_wav_path}")
                        return None

                if resp.status_code == 200:
                    audio_data = resp.content
                    print(f"   âœ… ç”ŸæˆæˆåŠŸ")
                else:
                    print(f"   âŒ APIé”™è¯¯: {resp.status_code}")
            except Exception as e:
                print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {e}")

        return {
            "index": index,
            "audio_data": audio_data,
            "bgm_filename": bgm_filename 
        }
# --- ä¸»æµæ°´çº¿ (å¼‚æ­¥åŒ…è£…) ---
async def process_pipeline_async(task_id: str, text: str, user_voice_map: dict):
    TASKS[task_id]["status"] = "analyzing"
    print("\nğŸ” [1/4] æ­£åœ¨åˆ†ææ–‡æœ¬...")
    dialogues = analyze_novel_roles_llm(text)
    if not dialogues:
        TASKS[task_id]["status"] = "failed"; return

    vm = VoiceManager(VOICE_POOL_DIR)
    TASKS[task_id]["status"] = "generating"
    TASKS[task_id]["total"] = len(dialogues)

    print(f"\nğŸš€ [2/4] å¯åŠ¨åŒå¡å¹¶å‘ç”Ÿæˆ! (æ€»å¥æ•°: {len(dialogues)})")
    
    semaphore = asyncio.Semaphore(4) 
    
    tasks = []
    for i, item in enumerate(dialogues):
        tasks.append(generate_segment_async(i, len(dialogues), item, user_voice_map, vm, semaphore))
    
    results = await asyncio.gather(*tasks)
    results = sorted(results, key=lambda x: x["index"] if x else -1)

    print("\nğŸ”¨ [3/4] æ­£åœ¨åˆå¹¶éŸ³é¢‘å¹¶æ·»åŠ BGM (å·²å¯ç”¨å¼ºåŠ›å»å™ª)...")
    final_segments = []
    
    last_role = None

    for res in results:
        if not res or not res["audio_data"]:
            continue
            
        try:
            # 1. åŸºç¡€å¤„ç†
            current_index = res["index"]
            original_item = dialogues[current_index]
            text_content = original_item["å°è¯"].strip()
            current_role = original_item["è§’è‰²"]

            import io
            speech_seg = AudioSegment.from_file(io.BytesIO(res["audio_data"]), format="wav")
            
            # 2. ç»Ÿä¸€éŸ³é‡
            speech_seg = match_target_amplitude(speech_seg, -20.0)
            
            # 3. èåˆ BGM
            bgm_filename = res["bgm_filename"]
            bgm_path = os.path.join(BGM_DIR, bgm_filename) if bgm_filename else None
            
            if bgm_path and os.path.exists(bgm_path):
                mixed_seg = mix_speech_with_bgm(speech_seg, bgm_path)
            else:
                mixed_seg = speech_seg
            
            # ========================================================
            # ğŸš¨ å…³é”®ä¿®å¤ï¼šæ¶ˆé™¤â€œå•µâ€å£°çš„ç»ˆææ‰‹æ®µ
            # åœ¨ BGM èåˆåï¼Œå¯¹æ•´ä½“è¿›è¡Œæ·¡å…¥æ·¡å‡ºã€‚
            # 20ms çš„æ·¡å…¥ + 30ms çš„æ·¡å‡ºï¼Œå¼ºåˆ¶æ³¢å½¢å½’é›¶ï¼Œæ¶ˆé™¤æ¥ç¼å™ªéŸ³ã€‚
            # ========================================================
            mixed_seg = mixed_seg.fade_in(20).fade_out(30)

            # 4. æ™ºèƒ½åœé¡¿è®¡ç®—
            pause_duration = 300 
            
            if text_content.endswith(("ï¼Œ", ",", "ã€")):
                pause_duration = 200 
            elif text_content.endswith(("ï¼", "!", "?", "ï¼Ÿ")):
                pause_duration = 500 
            elif text_content.endswith(("ã€‚", ".")):
                pause_duration = 450 
            elif text_content.endswith(("â€¦â€¦", "â€¦")):
                pause_duration = 700 
            
            if last_role and current_role != last_role:
                pause_duration += 150 
            
            if len(text_content) <= 2:
                pause_duration = 150 

            final_segments.append(mixed_seg)
            final_segments.append(AudioSegment.silent(duration=pause_duration))
            
            last_role = current_role
            TASKS[task_id]["progress"] = int((res["index"] / len(dialogues)) * 100)
            
        except Exception as e:
            print(f"åˆå¹¶å‡ºé”™: {e}")

    if not final_segments:
        TASKS[task_id]["status"] = "failed"; return

    full_audio = AudioSegment.empty()
    for seg in final_segments:
        full_audio += seg

    final_name = f"{task_id}.mp3"
    full_audio.export(os.path.join(OUTPUT_DIR, final_name), format="mp3")
    
    TASKS[task_id]["status"] = "completed"
    TASKS[task_id]["result_url"] = f"/download/{final_name}"
    TASKS[task_id]["progress"] = 100
    print(f"\nğŸ‰ [4/4] ä»»åŠ¡å®Œæˆï¼Œæ–‡ä»¶: {final_name}\n")

def run_async_pipeline(task_id, text, user_voice_map):
    asyncio.run(process_pipeline_async(task_id, text, user_voice_map))

# ================= 6. API æ¥å£ =================

@app.post("/analyze")
async def analyze_endpoint(file: UploadFile = File(...)):
    content = await file.read()
    text = content.decode("utf-8")
    dialogues = analyze_novel_roles_llm(text)
    unique_roles = set(item['è§’è‰²'] for item in dialogues)
    return {"roles": sorted(list(unique_roles), key=lambda x: 0 if x == "æ—ç™½" else 1)}

@app.post("/generate_step")
async def generate_step(request: Request, bg_tasks: BackgroundTasks):
    form = await request.form()
    file = form.get("file")
    if not file: return JSONResponse(400, {"error": "No file"})
    content = await file.read()
    text = content.decode("utf-8")
    
    user_voice_map = {}
    print("\nğŸ” [DEBUG] æ¥æ”¶å‰ç«¯è¡¨å•æ•°æ®:")
    for k, v in form.items():
        if k == "file": continue
        if k.startswith("custom_voice_") and hasattr(v, "filename") and v.filename:
            role = k.replace("custom_voice_", "")
            safe_name = f"{uuid.uuid4()}_{v.filename}"
            save_path = os.path.join(TEMP_VOICE_DIR, safe_name)
            with open(save_path, "wb") as f: shutil.copyfileobj(v.file, f)
            user_voice_map[role] = os.path.abspath(save_path)
            print(f"   ğŸ“‚ æ”¶åˆ°æ–‡ä»¶: [{role}] -> {v.filename}")
        elif k.startswith("preset_voice_") and isinstance(v, str) and v:
            role = k.replace("preset_voice_", "")
            path = os.path.join(VOICE_POOL_DIR, v)
            if os.path.exists(path):
                user_voice_map[role] = os.path.abspath(path)
                print(f"   ğŸµ æ”¶åˆ°é¢„è®¾: [{role}] -> {v}")

    task_id = str(uuid.uuid4())
    TASKS[task_id] = {"status": "pending", "progress": 0}
    bg_tasks.add_task(run_async_pipeline, task_id, text, user_voice_map)
    return {"task_id": task_id}

@app.get("/status/{task_id}")
def status(task_id: str): return TASKS.get(task_id, {})

@app.get("/download/{name}")
def download(name: str):
    path = os.path.join(OUTPUT_DIR, name)
    return FileResponse(path) if os.path.exists(path) else JSONResponse(404)

@app.get("/", response_class=HTMLResponse)
async def read_root():
    if os.path.exists("index.html"):
        with open("index.html", "r", encoding="utf-8") as f: return f.read()
    return "<h1>index.html Not Found</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8039)